{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "particular-probe",
   "metadata": {},
   "source": [
    "# Create the Image Tile Feature Sets\n",
    "In this notebook we create feature datasets from the image tiles that we previously generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "artistic-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from numpy import savez_compressed\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "civil-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define array names used in tile feature files\n",
    "FEATURES     = \"arr_0\"\n",
    "TILE_TAGS    = \"arr_1\"\n",
    "IMAGE_TAGS   = \"arr_2\"\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "norman-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file paths\n",
    "data_file_path     = \"./data/\"\n",
    "tile_features_path = \"\".join([data_file_path, \"features/\"]) \n",
    "feature_sets       = \"\".join([data_file_path, \"feature_sets/\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-trader",
   "metadata": {},
   "source": [
    "## Connect to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ahead-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DB connection between python and the file system\n",
    "conn = sqlite3.connect(''.join([data_file_path,\"/database/artist.db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-sydney",
   "metadata": {},
   "source": [
    "## Spliting the Datasets into __Train__, __Test__ & __Validation__ Subsets\n",
    "Both _genre_ & _artist_ feature sets have __211584__ (_3306x64_) entries. But, the _style_ feature set has __239488__ (_3742x64_). This is because an artwork may have more than one sytle associated with it. To allow the _feature sets_ to be split into _train_, _test_ & _validation_ subsets the _image_tag_ must be used to decide the final set destination. Here we perfor a double _train_test_split_ to achieve a splt ratio of approx: __70%__, __20%__ & __10%__.\n",
    "<br/>\n",
    "Create a simple query against the RDBMS to return a list of all _IMAGE_TAGs_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "weekly-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: select a list of unique image tags\n",
    "query_string = \"\"\"\n",
    "SELECT IMAGE_TAG\n",
    "FROM   ARTWORK_IMAGE\n",
    "\"\"\"\n",
    "\n",
    "# create the results dataframe\n",
    "i_tags = pd.read_sql_query(query_string, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-youth",
   "metadata": {},
   "source": [
    "Perform two random data splits to create the __TRAIN, TEST__ & __VALIDATE__ datasets selection in the approximate sizes : _70%_, _20%_ & _10%_. <br/> __NOTE:__ This is just the selection of which image tiles will be assigned to the three sets. This results in three lists of __IMAGE_TAGS__. The actual creation of the datasets happens further down the notebook. <br/>__NOTE:__ The splits are kept consistence with the use of the <code>RANDOM_STATE</code> being set throughout the project to value: __42__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "weighted-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the list of image tags into 3 sets:\n",
    "# train    ~70%\n",
    "# test     ~20%\n",
    "# validate ~10%\n",
    "\n",
    "# using the train test split we first extract our taining set which leaves a remainder\n",
    "train_tags   , remainder_tags, _, _ = train_test_split(i_tags        , i_tags        , test_size = 0.3, random_state = RANDOM_STATE)\n",
    "\n",
    "# now the reaminder is is split into a test and a validation set\n",
    "validate_tags, test_tags     , _, _ = train_test_split(remainder_tags, remainder_tags, test_size = 0.7, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-chase",
   "metadata": {},
   "source": [
    "Convert the three dataset selections into Numpy Arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "printable-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to reduce the search lookup process time we convert \n",
    "# the dataframes into numpy arrays\n",
    "train_tags    = train_tags[   \"image_tag\"].to_numpy()\n",
    "test_tags     = test_tags[    \"image_tag\"].to_numpy()\n",
    "validate_tags = validate_tags[\"image_tag\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-electronics",
   "metadata": {},
   "source": [
    "## Build the __Genre__ Feature Set Arrays\n",
    "\n",
    "Query the RDBMS to return a list of all __IMAGE_TAG__ values and the associated __GENRE__ catagories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "combined-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define query\n",
    "query_string = \"\"\"\n",
    "SELECT IMAGE_TAG,\n",
    "       GENRE\n",
    "FROM   GENRE         AS A,\n",
    "       ARTWORK       AS B,\n",
    "       ARTWORK_IMAGE AS C\n",
    "WHERE  A.ID = B.GENRE_ID\n",
    "AND    B.ID = C.ARTWORK_ID\n",
    "\"\"\"\n",
    "    \n",
    "# execute query\n",
    "genre_query_result = pd.read_sql_query(query_string, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-metallic",
   "metadata": {},
   "source": [
    "Here we loop through every every __IMAGE_TAG__ and for each one loop through all of its tiles. Identify if the tiles data should be in the __TRAIN, TEST__ or __VALIDATE__ dataset. Once the correct destination has been identified. The data is added to a set of array and finally written to one of three compressed data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "applied-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature set arrays\n",
    "features_tr   = []\n",
    "features_te   = []\n",
    "features_va   = []\n",
    "genre_tr      = []\n",
    "genre_te      = []\n",
    "genre_va      = []\n",
    "image_tag_tr  = []\n",
    "image_tag_te  = []\n",
    "image_tag_va  = []\n",
    "tile_index_tr = []\n",
    "tile_index_te = []\n",
    "tile_index_va = []\n",
    "\n",
    "# loop through selection rows\n",
    "for index, row in genre_query_result.iterrows():\n",
    "    \n",
    "    # set the tile index to 0\n",
    "    tile_index = 0\n",
    "\n",
    "    #read image tiles features\n",
    "    feature_data = np.load(\"\".join([tile_features_path, row[\"image_tag\"] ,\"_feature_set.npz\"]), allow_pickle=True)\n",
    "    \n",
    "    # loop through the feature sets of the 64 tiles per image\n",
    "    for tile_features in feature_data[FEATURES]:\n",
    "        \n",
    "        # check in smallest dataset and then second \n",
    "        # smallest dataset for speed\n",
    "        if row[\"image_tag\"] in validate_tags:\n",
    "            # append data to feature set arrays\n",
    "            features_va.append(tile_features[0]  )\n",
    "            genre_va.append(     row[\"genre\"]    )\n",
    "            image_tag_va.append( row[\"image_tag\"])\n",
    "            tile_index_va.append(tile_index      )\n",
    "        elif row[\"image_tag\"] in test_tags:\n",
    "            # append data to feature set arrays\n",
    "            features_te.append(tile_features[0]  )\n",
    "            genre_te.append(     row[\"genre\"]    )\n",
    "            image_tag_te.append( row[\"image_tag\"])\n",
    "            tile_index_te.append(tile_index      )\n",
    "        else:\n",
    "            # append data to feature set arrays\n",
    "            features_tr.append(tile_features[0]  )\n",
    "            genre_tr.append(     row[\"genre\"]    )\n",
    "            image_tag_tr.append( row[\"image_tag\"])\n",
    "            tile_index_tr.append(tile_index      )\n",
    "            \n",
    "        # incremet the tile index by 1\n",
    "        tile_index += 1\n",
    "            \n",
    " # write files\n",
    "savez_compressed(\"\".join([feature_sets,\"genre_train_features\"    ]),  features_tr, genre_tr, image_tag_tr, tile_index_tr)\n",
    "savez_compressed(\"\".join([feature_sets,\"genre_test_features\"     ]),  features_te, genre_te, image_tag_te, tile_index_te)\n",
    "savez_compressed(\"\".join([feature_sets,\"genre_validation_features\"]), features_va, genre_va, image_tag_va, tile_index_va)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-employee",
   "metadata": {},
   "source": [
    "## Build the __Style__ Feature Set Arrays\n",
    "\n",
    "Query the RDBMS to return a list of all __IMAGE_TAG__ values and the associated __STYLE__ catagories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "statewide-restriction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we generate additonal records here becaus an artwork can be listed with more than one style\n",
    "query_string = \"\"\"\n",
    "SELECT IMAGE_TAG,\n",
    "       STYLE\n",
    "FROM   STYLE         AS A,\n",
    "       ARTWORK_STYLE AS B,\n",
    "       ARTWORK_IMAGE AS C\n",
    "WHERE   B.STYLE_ID  = A.ID   \n",
    "AND    C.ARTWORK_ID = B.ARTWORK_ID\n",
    "\"\"\"\n",
    "    \n",
    "style_query_result = pd.read_sql_query(query_string, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-peace",
   "metadata": {},
   "source": [
    "Here we loop through every every __IMAGE_TAG__ and for each one loop through all of its tiles. Identify if the tiles data should be in the __TRAIN, TEST__ or __VALIDATE__ dataset. Once the correct destination has been identified. The data is added to a set of array and finally written to one of three compressed data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "later-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature set arrays\n",
    "features_tr   = []\n",
    "features_te   = []\n",
    "features_va   = []\n",
    "style_tr      = []\n",
    "style_te      = []\n",
    "style_va      = []\n",
    "image_tag_tr  = []\n",
    "image_tag_te  = []\n",
    "image_tag_va  = []\n",
    "tile_index_tr = []\n",
    "tile_index_te = []\n",
    "tile_index_va = []\n",
    "\n",
    "# loop through selection rows\n",
    "for index, row in style_query_result.iterrows():\n",
    "    \n",
    "    # set the tile index to 0\n",
    "    tile_index = 0\n",
    "\n",
    "    #read image tiles features\n",
    "    feature_data = np.load(\"\".join([tile_features_path, row[\"image_tag\"] ,\"_feature_set.npz\"]), allow_pickle=True)\n",
    "    \n",
    "    # loop through the feature sets of the 64 tiles per image\n",
    "    for tile_features in feature_data[FEATURES]:\n",
    "\n",
    "        # check in smallest dataset and then second \n",
    "        # smallest dataset for speed\n",
    "        if row[\"image_tag\"] in validate_tags:\n",
    "            # append data to feature set arrays\n",
    "            features_va.append(tile_features[0]  )\n",
    "            style_va.append(     row[\"style\"]    )\n",
    "            image_tag_va.append( row[\"image_tag\"])\n",
    "            tile_index_va.append(tile_index      )\n",
    "        elif row[\"image_tag\"] in test_tags:\n",
    "            # append data to feature set arrays\n",
    "            features_te.append(tile_features[0]  )\n",
    "            style_te.append(     row[\"style\"]    )\n",
    "            image_tag_te.append( row[\"image_tag\"])\n",
    "            tile_index_te.append(tile_index      )\n",
    "        else:\n",
    "            # append data to feature set arrays\n",
    "            features_tr.append(tile_features[0]  )\n",
    "            style_tr.append(     row[\"style\"]    )\n",
    "            image_tag_tr.append( row[\"image_tag\"])\n",
    "            tile_index_tr.append(tile_index      )\n",
    "            \n",
    "        # incremet the tile index by 1\n",
    "        tile_index += 1\n",
    "        \n",
    " # write files\n",
    "savez_compressed(\"\".join([feature_sets,\"style_train_features\"    ]),  features_tr, style_tr, image_tag_tr, tile_index_tr)\n",
    "savez_compressed(\"\".join([feature_sets,\"style_test_features\"     ]),  features_te, style_te, image_tag_te, tile_index_te)\n",
    "savez_compressed(\"\".join([feature_sets,\"style_validation_features\"]), features_va, style_va, image_tag_va, tile_index_va)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-quarter",
   "metadata": {},
   "source": [
    "## Build the __Artist__ Feature Set Arrays\n",
    "\n",
    "Query the RDBMS to return a list of all __IMAGE_TAG__ values and the associated __ARTIST__ names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "related-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we generate additonal records here becaus an artwork can be listed with more than one style\n",
    "query_string = \"\"\"\n",
    "SELECT IMAGE_TAG,\n",
    "       NAME\n",
    "FROM   ARTWORK       AS A,\n",
    "       ARTIST        AS B,\n",
    "       ARTWORK_IMAGE AS C\n",
    "WHERE  A.ARTIST_ID = B.ID   \n",
    "AND    A.ID        = C.ARTWORK_ID\n",
    "\"\"\"\n",
    "    \n",
    "artist_query_result = pd.read_sql_query(query_string, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-trade",
   "metadata": {},
   "source": [
    "Here we loop through every every __IMAGE_TAG__ and for each one loop through all of its tiles. Identify if the tiles data should be in the __TRAIN, TEST__ or __VALIDATE__ dataset. Once the correct destination has been identified. The data is added to a set of array and finally written to one of three compressed data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "tamil-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature set arrays\n",
    "features_tr   = []\n",
    "features_te   = []\n",
    "features_va   = []\n",
    "artist_tr     = []\n",
    "artist_te     = []\n",
    "artist_va     = []\n",
    "image_tag_tr  = []\n",
    "image_tag_te  = []\n",
    "image_tag_va  = []\n",
    "tile_index_tr = []\n",
    "tile_index_te = []\n",
    "tile_index_va = []\n",
    "\n",
    "# loop through selection rows\n",
    "for index, row in artist_query_result.iterrows():\n",
    "    \n",
    "    # set the tile index to 0\n",
    "    tile_index = 0\n",
    "\n",
    "    #read image tiles features\n",
    "    feature_data = np.load(\"\".join([tile_features_path, row[\"image_tag\"] ,\"_feature_set.npz\"]), allow_pickle=True)\n",
    "    \n",
    "    # loop through the feature sets of the 64 tiles per image\n",
    "    for tile_features in feature_data[FEATURES]:\n",
    "\n",
    "        # check in smallest dataset and then second \n",
    "        # smallest dataset for speed\n",
    "        if row[\"image_tag\"] in validate_tags:\n",
    "            # append data to feature set arrays\n",
    "            features_va.append(tile_features[0]  )\n",
    "            artist_va.append(    row[\"name\"]     )\n",
    "            image_tag_va.append( row[\"image_tag\"])\n",
    "            tile_index_va.append(tile_index      )\n",
    "        elif row[\"image_tag\"] in test_tags:\n",
    "            # append data to feature set arrays\n",
    "            features_te.append(tile_features[0]  )\n",
    "            artist_te.append(    row[\"name\"]     )\n",
    "            image_tag_te.append( row[\"image_tag\"])\n",
    "            tile_index_te.append(tile_index      )\n",
    "            \n",
    "        else:\n",
    "            # append data to feature set arrays\n",
    "            features_tr.append(tile_features[0]  )\n",
    "            artist_tr.append(    row[\"name\"]     )\n",
    "            image_tag_tr.append( row[\"image_tag\"])\n",
    "            tile_index_tr.append(tile_index      )\n",
    "            \n",
    "        # incremet the tile index by 1\n",
    "        tile_index += 1\n",
    "            \n",
    " # write files\n",
    "savez_compressed(\"\".join([feature_sets,\"artist_train_features\"    ]),  features_tr, artist_tr, image_tag_tr, tile_index_tr)\n",
    "savez_compressed(\"\".join([feature_sets,\"artist_test_features\"     ]),  features_te, artist_te, image_tag_te, tile_index_te)\n",
    "savez_compressed(\"\".join([feature_sets,\"artist_validation_features\"]), features_va, artist_va, image_tag_va, tile_index_va)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-virus",
   "metadata": {},
   "source": [
    "__NOTE:__ the building of the _feature sets_ could have been reduced to a _helper_ function and called with parameters for the 3 sets. But, for speed I decided to leave them as they are. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
